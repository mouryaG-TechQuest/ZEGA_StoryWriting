name: ZEGA-training-pipeline-v1
architecture: teacher-student-ensemble
student_model: zega-core (chroma-rag)
teacher_models:
  - name: gemini-pro
    provider: google
    enabled: true
  - name: llama3-70b
    provider: groq
    enabled: false # Enable if GROQ_API_KEY is present
  - name: mistral-large
    provider: huggingface
    enabled: false # Enable if HF_TOKEN is present
adapters:
  type: prompt-tuning
  rank: 8
  init: zeros
personalization:
  style_vector_dim: 512
  update_strategy:
    - local_online_update: true
    - client_batch_size: 4
federated:
  enabled: false
  secure_aggregation: true
  dp_epsilon: 8.0
continual_learning:
  replay_buffer_size: 10000
  ewc_lambda: 0.1
rag:
  vector_db: "chroma"
  chunk_size: 256
evaluation:
  strategy: "llm-as-a-judge" # Use strongest teacher to evaluate student
  holdout_sets:
    - name: benchmark_suite_v1
    - name: user_simulated_holdout
